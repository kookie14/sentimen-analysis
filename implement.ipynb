{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install pyvi\n","!pip install unidecode\n","!pip install emoji"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:30:07.331598Z","iopub.execute_input":"2022-12-10T21:30:07.331834Z","iopub.status.idle":"2022-12-10T21:30:40.569567Z","shell.execute_reply.started":"2022-12-10T21:30:07.331787Z","shell.execute_reply":"2022-12-10T21:30:40.568588Z"},"trusted":true,"id":"cjEfxvFP-DYg","outputId":"6e0f7ccb-cbfe-4e94-bf9c-24060e904ae4"},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting pyvi\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/27/27ffee2663f42430cf3434da963f04224fec157b90799fe9e92a3564c1a6/pyvi-0.1.1-py2.py3-none-any.whl (8.5MB)\n\u001b[K     |████████████████████████████████| 8.5MB 902kB/s eta 0:00:01\n\u001b[?25hCollecting sklearn-crfsuite (from pyvi)\n  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from pyvi) (0.21.2)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.6/site-packages/tabulate-0.8.3-py3.6.egg (from sklearn-crfsuite->pyvi) (0.8.3)\nRequirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.6/site-packages (from sklearn-crfsuite->pyvi) (4.32.1)\nCollecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->pyvi)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/b9/b6f48d74e10136ccfafbadcae751f3e81d143b40847d0f20728026783834/python-crfsuite-0.9.8.tar.gz (440kB)\n\u001b[K     |████████████████████████████████| 440kB 43.5MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sklearn-crfsuite->pyvi) (1.12.0)\nRequirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->pyvi) (1.16.4)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->pyvi) (0.13.2)\nRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->pyvi) (1.2.1)\nBuilding wheels for collected packages: python-crfsuite\n  Building wheel for python-crfsuite (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/fa/ee/3afb15958ad26f3aef88d61c316b4d1af8a97660aa24e6e6d7\nSuccessfully built python-crfsuite\nInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\nSuccessfully installed python-crfsuite-0.9.8 pyvi-0.1.1 sklearn-crfsuite-0.3.6\nRequirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (1.1.1)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.6/site-packages (0.5.2)\n","output_type":"stream"}]},{"cell_type":"code","source":["#Ignoring the warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","#Importing the required libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re, string, unicodedata\n","import nltk\n","import os\n","from nltk import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers.pooling import GlobalMaxPooling1D\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.models import load_model\n","from keras.layers import *\n","from keras import backend\n","from sklearn.metrics import f1_score, confusion_matrix\n","import tensorflow as tf\n","\n","from pyvi import ViTokenizer\n","from pyvi import ViUtils"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:30:40.572908Z","iopub.execute_input":"2022-12-10T21:30:40.573207Z","iopub.status.idle":"2022-12-10T21:30:43.320696Z","shell.execute_reply.started":"2022-12-10T21:30:40.573146Z","shell.execute_reply":"2022-12-10T21:30:43.319779Z"},"trusted":true,"id":"eqfUewRv-DYk","outputId":"8c0c61d5-9634-470d-a143-6c954684da18"},"execution_count":null,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"markdown","source":["### Importing the dataset\n","***\n","\n","The dataset: 'data1.csv' is read and loaded as pandas dataframe.  \n","Let's have a look at the data"],"metadata":{"id":"g80NRZwA-DYm"}},{"cell_type":"code","source":["#Importing the dataset\n","dataset = pd.read_csv('/kaggle/input/cleandata/data1.csv')\n","df = pd.read_csv('/kaggle/input/int3405-sentiment-analysis-problem/test.csv')\n","dataset.head()"],"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-12-10T21:30:43.322170Z","iopub.execute_input":"2022-12-10T21:30:43.322497Z","iopub.status.idle":"2022-12-10T21:30:43.900211Z","shell.execute_reply.started":"2022-12-10T21:30:43.322446Z","shell.execute_reply":"2022-12-10T21:30:43.899329Z"},"trusted":true,"id":"bd6vyP6r-DYn","outputId":"312b2313-43d3-4486-efc5-367868e9ddd2"},"execution_count":null,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                             Comment  Rating\n0  Xôi dẻo, đồ ăn đậm vị. Hộp xôi được lót lá trô...       1\n1  Gọi ship 1 xuất cari gà bánh naan và 3 miếng g...       0\n2  Thời tiết lạnh như này, cả nhà rủ nhau đến leg...       1\n3  Em có đọc review thấy mng bảo trà sữa nướng đề...       0\n4  Đồ ăn rất ngon, nhà hàng cũng rất đẹp, tất cả ...       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n      <th>Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Xôi dẻo, đồ ăn đậm vị. Hộp xôi được lót lá trô...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gọi ship 1 xuất cari gà bánh naan và 3 miếng g...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Thời tiết lạnh như này, cả nhà rủ nhau đến leg...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Em có đọc review thấy mng bảo trà sữa nướng đề...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Đồ ăn rất ngon, nhà hàng cũng rất đẹp, tất cả ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["def lowerCase(data):\n","    data = data.lower()\n","    return data\n","def remove_punctuations(data):\n","    punct_tag = re.compile(r'[^\\w\\s]')\n","    data = punct_tag.sub(r'',data)\n","    return data\n","def remove_html(data):\n","    html_tag=re.compile(r'<.*?>')\n","    data=html_tag.sub(r'',data)\n","    return data\n","def remove_url(data):\n","    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n","    data=url_clean.sub(r'',data)\n","    return data\n","def remove_n(data):\n","    html_tag=re.compile(r'\\n')\n","    data=html_tag.sub(r' ',data)\n","    return data\n","def remove_emoji(data):\n","    emoji_clean= re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           u\"\\U00002702-\\U000027B0\"\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    data=emoji_clean.sub(r'',data)\n","    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n","    data=url_clean.sub(r'',data)\n","    return data\n","def remove_x(data):\n","    html_tag=re.compile(r'\\xa0')\n","    data=html_tag.sub(r' ',data)\n","    return data"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:30:43.901788Z","iopub.execute_input":"2022-12-10T21:30:43.902093Z","iopub.status.idle":"2022-12-10T21:30:43.913425Z","shell.execute_reply.started":"2022-12-10T21:30:43.902044Z","shell.execute_reply":"2022-12-10T21:30:43.912641Z"},"trusted":true,"id":"oa7M1www-DYn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['Comment'] = dataset['Comment'].apply(lambda z: lowerCase(str(z)))\n","\n","dataset['Comment'] = dataset['Comment'].apply(lambda z: remove_punctuations(z))\n","\n","dataset['Comment'] = dataset['Comment'].apply(lambda z: remove_n(z))\n","\n","dataset['Comment'] =  dataset['Comment'].apply(lambda z: remove_x(z))\n","\n","dataset['Comment'] =  dataset['Comment'].apply(lambda z: remove_html(z))\n","\n","dataset['Comment'] =  dataset['Comment'].apply(lambda z: remove_url(z))\n","\n","dataset['Comment'] =  dataset['Comment'].apply(lambda z: remove_emoji(z))"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:30:43.916567Z","iopub.execute_input":"2022-12-10T21:30:43.916842Z","iopub.status.idle":"2022-12-10T21:30:45.528956Z","shell.execute_reply.started":"2022-12-10T21:30:43.916784Z","shell.execute_reply":"2022-12-10T21:30:45.528123Z"},"trusted":true,"id":"-xXDYL7U-DYo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.info()"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:30:45.532037Z","iopub.execute_input":"2022-12-10T21:30:45.532579Z","iopub.status.idle":"2022-12-10T21:30:45.546588Z","shell.execute_reply.started":"2022-12-10T21:30:45.532370Z","shell.execute_reply":"2022-12-10T21:30:45.545718Z"},"trusted":true,"id":"neBQpYM_-DYo","outputId":"d2b61e46-52be-442a-c139-fd041e0cfd47"},"execution_count":null,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 39071 entries, 0 to 39070\nData columns (total 2 columns):\nComment    39071 non-null object\nRating     39071 non-null int64\ndtypes: int64(1), object(1)\nmemory usage: 610.6+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":["X_train = dataset['Comment'].values\n","y_train = dataset['Rating'].values\n","X_train = np.array(X_train)\n","y_train = np.array(y_train)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:30:45.548583Z","iopub.execute_input":"2022-12-10T21:30:45.549011Z","iopub.status.idle":"2022-12-10T21:30:45.554170Z","shell.execute_reply.started":"2022-12-10T21:30:45.548842Z","shell.execute_reply":"2022-12-10T21:30:45.553315Z"},"trusted":true,"id":"pEB2YaP0-DYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train[:2])\n","y_train[:2]"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:30:45.555911Z","iopub.execute_input":"2022-12-10T21:30:45.556344Z","iopub.status.idle":"2022-12-10T21:30:45.566362Z","shell.execute_reply.started":"2022-12-10T21:30:45.556158Z","shell.execute_reply":"2022-12-10T21:30:45.565319Z"},"trusted":true,"id":"EYHIUkAa-DYq","outputId":"0330b934-ea20-4bb5-af69-939f371636f4"},"execution_count":null,"outputs":[{"name":"stdout","text":"['xôi dẻo đồ ăn đậm vị hộp xôi được lót lá trông rất thích'\n 'gọi ship 1 xuất cari gà bánh naan và 3 miếng gà nướngđược tặng 1 coca đồ ăn khá ngon tổng 210k được giảm 50k còn 160k tuy nhiên gọi 3 miếng gà thì thiếu 1 miếng mà kể cả đó đủ ba miếng thì khẩu phần vẫn là quá ít so với giá 120k 1 suất']\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([1, 0])"},"metadata":{}}]},{"cell_type":"markdown","source":["### Text Preprocessing\n","***\n","Preprocessing the text so as to have a better data for our model.  \n","It comprises of steps such as removing non-ASCII characters, removing HTML tags, converting to lower-case, lemmatizing."],"metadata":{"id":"JjmYHS2r-DYq"}},{"cell_type":"code","source":["\n","def clean_text(X,y):\n","    idx = 0\n","    y_train = []\n","    processed = []\n","    for text in X:\n","        text = list(tf.keras.preprocessing.text.text_to_word_sequence(text))\n","        text = \" \".join(text)\n","        input_text_pre_no_accent = str(ViUtils.remove_accents(text).decode(\"utf-8\"))\n","        input_text_pre_accent = ViTokenizer.tokenize(text)\n","        input_text_pre_no_accent = ViTokenizer.tokenize(input_text_pre_no_accent)\n","        processed.append(input_text_pre_accent)\n","        processed.append(input_text_pre_no_accent)\n","        y_train.append(y[idx])\n","        y_train.append(y[idx])\n","        idx += 1\n","    return processed,y_train"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:30:45.567823Z","iopub.execute_input":"2022-12-10T21:30:45.568524Z","iopub.status.idle":"2022-12-10T21:30:45.577340Z","shell.execute_reply.started":"2022-12-10T21:30:45.568086Z","shell.execute_reply":"2022-12-10T21:30:45.576617Z"},"trusted":true,"id":"dU0iOFyH-DYr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preprocessing the Training Set and Test set"],"metadata":{"id":"gOHBY1fJ-DYr"}},{"cell_type":"code","source":["y_train[0]"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:30:45.578935Z","iopub.execute_input":"2022-12-10T21:30:45.579202Z","iopub.status.idle":"2022-12-10T21:30:45.588941Z","shell.execute_reply.started":"2022-12-10T21:30:45.579130Z","shell.execute_reply":"2022-12-10T21:30:45.587996Z"},"trusted":true,"id":"IxY_syvs-DYr","outputId":"dfe385cc-2c89-4de6-937f-022f4059a4b6"},"execution_count":null,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":["X_train_final,y_train = clean_text(X_train,y_train)\n","# X_test_final,y_test = clean_text(X_test,y_test)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:30:45.590161Z","iopub.execute_input":"2022-12-10T21:30:45.590441Z","iopub.status.idle":"2022-12-10T21:32:37.724774Z","shell.execute_reply.started":"2022-12-10T21:30:45.590397Z","shell.execute_reply":"2022-12-10T21:32:37.723911Z"},"trusted":true,"id":"y7YIaSzG-DYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(X_train_final)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:32:37.726204Z","iopub.execute_input":"2022-12-10T21:32:37.726506Z","iopub.status.idle":"2022-12-10T21:32:37.732305Z","shell.execute_reply.started":"2022-12-10T21:32:37.726459Z","shell.execute_reply":"2022-12-10T21:32:37.731349Z"},"trusted":true,"id":"BtYibXyO-DYs","outputId":"1188efa7-fdd0-402a-e8c0-28733487f836"},"execution_count":null,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"78142"},"metadata":{}}]},{"cell_type":"code","source":["print(X_train_final[:2])\n","print(y_train[:2])"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:32:37.733939Z","iopub.execute_input":"2022-12-10T21:32:37.734428Z","iopub.status.idle":"2022-12-10T21:32:37.743320Z","shell.execute_reply.started":"2022-12-10T21:32:37.734226Z","shell.execute_reply":"2022-12-10T21:32:37.742283Z"},"trusted":true,"id":"PScnN3UW-DYs","outputId":"1a8bda40-8c47-41f5-c3d2-81e105aea012"},"execution_count":null,"outputs":[{"name":"stdout","text":"['xôi dẻo đồ ăn_đậm vị hộp xôi được lót lá trông rất thích', 'xoi deo do an dam vi hop xoi duoc lot la trong rat thich']\n[1, 1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":["### Attention Layer\n","***\n","\n","The basic concept of attention is that not all words contribute equally to the meaning of a sentence. Hence, their contribution must be weighted.  \n","How attention works is, it basically extracts words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector."],"metadata":{"id":"QIoDfkVs-DYs"}},{"cell_type":"code","source":["# Attention Layer\n","class AttentionWithContext(Layer):\n","    \"\"\"\n","    Attention operation, with a context/query vector, for temporal data.\n","    Supports Masking.\n","    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n","    \"Hierarchical Attention Networks for Document Classification\"\n","    by using a context vector to assist the attention\n","    # Input shape\n","        3D tensor with shape: `(samples, steps, features)`.\n","    # Output shape\n","        2D tensor with shape: `(samples, features)`.\n","    How to use:\n","    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n","    The dimensions are inferred based on the output shape of the RNN.\n","    Note: The layer has been tested with Keras 2.0.6\n","    Example:\n","        model.add(LSTM(64, return_sequences=True))\n","        model.add(AttentionWithContext())\n","        # next add a Dense layer (for classification/regression) or whatever...\n","    \"\"\"\n","\n","    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, u_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","\n","        self.supports_masking = True\n","        self.init = initializers.get('glorot_uniform')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.u_regularizer = regularizers.get(u_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.u_constraint = constraints.get(u_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        super(AttentionWithContext, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        if self.bias:\n","            self.b = self.add_weight((input_shape[-1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","\n","        self.u = self.add_weight((input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_u'.format(self.name),\n","                                 regularizer=self.u_regularizer,\n","                                 constraint=self.u_constraint)\n","\n","        super(AttentionWithContext, self).build(input_shape)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        # do not pass the mask to the next layers\n","        return None\n","\n","    def call(self, x, mask=None):\n","        uit = dot_product(x, self.W)\n","\n","        if self.bias:\n","            uit += self.b\n","\n","        uit = K.tanh(uit)\n","        ait = dot_product(uit, self.u)\n","\n","        a = K.exp(ait)\n","\n","        # apply mask after the exp. will be re-normalized next\n","        if mask is not None:\n","            # Cast the mask to floatX to avoid float64 upcasting in theano\n","            a *= K.cast(mask, K.floatx())\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], input_shape[-1]\n","\n","def dot_product(x, kernel):\n","    if K.backend() == 'tensorflow':\n","        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n","    else:\n","        return K.dot(x, kernel)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:32:37.745012Z","iopub.execute_input":"2022-12-10T21:32:37.745594Z","iopub.status.idle":"2022-12-10T21:32:37.765855Z","shell.execute_reply.started":"2022-12-10T21:32:37.745390Z","shell.execute_reply":"2022-12-10T21:32:37.765163Z"},"trusted":true,"id":"eZD2AhLw-DYt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Paramenter needed  \n"],"metadata":{"id":"7liLtNhA-DYt"}},{"cell_type":"code","source":["#Tokenization and Padding\n","vocab_size = 60000\n","maxlen = 250\n","encode_dim = 20\n","batch_size = 32\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train_final)\n","tokenized_word_list = tokenizer.texts_to_sequences(X_train_final)\n","X_train_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:32:37.767394Z","iopub.execute_input":"2022-12-10T21:32:37.767828Z","iopub.status.idle":"2022-12-10T21:32:51.343293Z","shell.execute_reply.started":"2022-12-10T21:32:37.767658Z","shell.execute_reply":"2022-12-10T21:32:51.342401Z"},"trusted":true,"id":"SlS0CZUL-DYt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.shape(X_train_padded))\n","print(np.shape(X_train_final))"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:32:51.344743Z","iopub.execute_input":"2022-12-10T21:32:51.345033Z","iopub.status.idle":"2022-12-10T21:32:52.941181Z","shell.execute_reply.started":"2022-12-10T21:32:51.344985Z","shell.execute_reply":"2022-12-10T21:32:52.940341Z"},"trusted":true,"id":"-SBC2Pyx-DYt","outputId":"6d7b1d45-6fcc-4738-e3ab-e26baf4d4099"},"execution_count":null,"outputs":[{"name":"stdout","text":"(78142, 250)\n(78142,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":["**EarlyStopping**  \n","It can be used to prevent overfitting.It basically waits a few epochs (5), monitoring the loss for the validation dataset.If the loss doesn't decrease for 2 epochs, it stops the training process.\n","\n","**ModelCheckpoint**  \n","It is used for saving the best model during training. After each epoch, it takes a look at the Validation accuracy, if it improves globally, this is the best model we have seen till now during the training process and hence, saves it."],"metadata":{"id":"DIOJ7Dkq-DYu"}},{"cell_type":"code","source":["#EarlyStopping and ModelCheckpoint\n","\n","es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\n","mc = ModelCheckpoint('model_best.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:32:52.942645Z","iopub.execute_input":"2022-12-10T21:32:52.943140Z","iopub.status.idle":"2022-12-10T21:32:52.953271Z","shell.execute_reply.started":"2022-12-10T21:32:52.943089Z","shell.execute_reply":"2022-12-10T21:32:52.952470Z"},"trusted":true,"id":"fwFc4JSj-DYu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Building the Model\n","***\n","The model used comprises of BiDirectional LSTM with Attention layer on top of it, followed by a dense layer and finally a dense layer with sigmoid activation function to get the sentiment or the class.  \n","Optimiser used is ADAM"],"metadata":{"id":"xL-GljaD-DYu"}},{"cell_type":"code","source":["#Building the model\n","model = Sequential()\n","embed = Embedding(input_dim = vocab_size, output_dim = 20, input_length = X_train_padded.shape[1], dropout = 0.1) \n","model.add(embed)\n","model.add(Bidirectional(CuDNNLSTM(200, return_sequences = True)))\n","model.add(Dropout(0.1))\n","# model.add(Bidirectional(CuDNNLSTM(100,return_sequences=False)))\n","# model.add(Bidirectional(CuDNNLSTM(200,return_sequences=True)))\n","model.add(Dropout(0.1))\n","model.add(AttentionWithContext())\n","model.add(Dropout(0.1))\n","model.add(Dense(512))\n","model.add(LeakyReLU(alpha=0.1))\n","model.add(Dense(256))\n","model.add(LeakyReLU(alpha=0.1))\n","model.add(Dense(1, activation = 'sigmoid'))\n","model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","model.summary()"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:32:52.954862Z","iopub.execute_input":"2022-12-10T21:32:52.955691Z","iopub.status.idle":"2022-12-10T21:32:52.961902Z","shell.execute_reply.started":"2022-12-10T21:32:52.955128Z","shell.execute_reply":"2022-12-10T21:32:52.961057Z"},"trusted":true,"id":"sW57LG0J-DYu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from keras.utils.vis_utils import plot_model\n","# plot_model(model, to_file = 'model_plot.png', show_shapes = True, show_layer_names = True)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:32:52.963460Z","iopub.execute_input":"2022-12-10T21:32:52.963931Z","iopub.status.idle":"2022-12-10T21:32:52.971616Z","shell.execute_reply.started":"2022-12-10T21:32:52.963753Z","shell.execute_reply":"2022-12-10T21:32:52.970643Z"},"trusted":true,"id":"cUCe020r-DYu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing\n","***\n","Converting the test data into sequences of integers and padding them.  \n","Loading the best model and calculating the accuracy"],"metadata":{"id":"4KGVyE_B-DYv"}},{"cell_type":"code","source":["from keras.models import load_model\n","modelload = load_model('/kaggle/input/best-model/model_best (1).h5', custom_objects = {\"AttentionWithContext\" : AttentionWithContext, \"backend\" : backend})"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:32:53.008670Z","iopub.execute_input":"2022-12-10T21:32:53.008986Z","iopub.status.idle":"2022-12-10T21:32:59.779153Z","shell.execute_reply.started":"2022-12-10T21:32:53.008937Z","shell.execute_reply":"2022-12-10T21:32:59.778323Z"},"trusted":true,"id":"7U5lkOVr-DYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# df = pd.read_csv('/kaggle/input/int3405-sentiment-analysis-problem/test.csv')\n","df['Comment'] = df['Comment'].apply(lambda z: lowerCase(str(z)))\n","\n","df['Comment'] = df['Comment'].apply(lambda z: remove_punctuations(z))\n","\n","df['Comment'] = df['Comment'].apply(lambda z: remove_n(z))\n","\n","df['Comment'] = df['Comment'].apply(lambda z: remove_x(z))\n","\n","df['Comment'] =  df['Comment'].apply(lambda z: remove_html(z))\n","\n","df['Comment'] =  df['Comment'].apply(lambda z: remove_url(z))\n","\n","df['Comment'] =  df['Comment'].apply(lambda z: remove_emoji(z))\n","\n","data_test = pd.DataFrame({'input':df['Comment'],'id':df[\"RevId\"]})\n","\n","X_test = data_test['input'].values\n"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:33:32.178030Z","iopub.execute_input":"2022-12-10T21:33:32.178363Z","iopub.status.idle":"2022-12-10T21:33:32.474107Z","shell.execute_reply.started":"2022-12-10T21:33:32.178303Z","shell.execute_reply":"2022-12-10T21:33:32.473268Z"},"trusted":true,"id":"KJE_T0tX-DYw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_text_test(X):\n","    processed = []\n","    for text in X:\n","        text = list(tf.keras.preprocessing.text.text_to_word_sequence(str(text)))\n","        text = \" \".join(text)\n","        input_text_pre_no_accent = str(ViUtils.remove_accents(text).decode(\"utf-8\"))\n","        input_text_pre_accent = ViTokenizer.tokenize(text)\n","        processed.append(input_text_pre_accent)\n","    return processed\n","\n","X_test_final = clean_text_test(X_test)\n","tokenized_word_list = tokenizer.texts_to_sequences(X_test_final)\n","X_test_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:33:55.894345Z","iopub.execute_input":"2022-12-10T21:33:55.894669Z","iopub.status.idle":"2022-12-10T21:34:04.338866Z","shell.execute_reply.started":"2022-12-10T21:33:55.894614Z","shell.execute_reply":"2022-12-10T21:34:04.337905Z"},"trusted":true,"id":"nENLVfT3-DYw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = modelload.predict(X_test_padded)\n","print(y_pred[:10])\n","print(np.shape(y_pred))"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:34:04.340785Z","iopub.execute_input":"2022-12-10T21:34:04.341234Z","iopub.status.idle":"2022-12-10T21:34:08.662939Z","shell.execute_reply.started":"2022-12-10T21:34:04.341047Z","shell.execute_reply":"2022-12-10T21:34:08.661945Z"},"trusted":true,"id":"Jmd5eqo4-DYw","outputId":"04673c07-9fad-4fc1-979d-40163368e07f"},"execution_count":null,"outputs":[{"name":"stdout","text":"(5103, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":["print(X_test_final[9])"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:33:00.170767Z","iopub.status.idle":"2022-12-10T21:33:00.171452Z"},"trusted":true,"id":"KwZ39R-p-DYw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_test_final[12])"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:33:00.172813Z","iopub.status.idle":"2022-12-10T21:33:00.173492Z"},"trusted":true,"id":"oHix_Uu5-DYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_test_final[1])"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:33:00.174890Z","iopub.status.idle":"2022-12-10T21:33:00.176520Z"},"trusted":true,"id":"9T3MLGBF-DYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_submission = pd.DataFrame({'RevId': np.array(df[\"RevId\"]), 'Rating': np.array(y_pred).reshape(5103)})\n","my_submission.to_csv('submission.csv', index=False)"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:34:38.170266Z","iopub.execute_input":"2022-12-10T21:34:38.170588Z","iopub.status.idle":"2022-12-10T21:34:38.224664Z","shell.execute_reply.started":"2022-12-10T21:34:38.170534Z","shell.execute_reply":"2022-12-10T21:34:38.223904Z"},"trusted":true,"id":"OJwerRdS-DYx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["my_submission[:5]"],"metadata":{"execution":{"iopub.status.busy":"2022-12-10T21:34:45.150932Z","iopub.execute_input":"2022-12-10T21:34:45.151260Z","iopub.status.idle":"2022-12-10T21:34:45.162551Z","shell.execute_reply.started":"2022-12-10T21:34:45.151205Z","shell.execute_reply":"2022-12-10T21:34:45.161741Z"},"trusted":true,"id":"GfSjraMD-DYy","outputId":"865c5edb-ccc6-40a5-9306-2b6f4bbeb419"},"execution_count":null,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"     RevId    Rating\n0   781115  0.038603\n1  1219481  0.272514\n2  1703765  0.990643\n3  4870346  0.110394\n4  2638711  0.981124","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RevId</th>\n      <th>Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>781115</td>\n      <td>0.038603</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1219481</td>\n      <td>0.272514</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1703765</td>\n      <td>0.990643</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4870346</td>\n      <td>0.110394</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2638711</td>\n      <td>0.981124</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"8Jhe98pa-DYy"},"execution_count":null,"outputs":[]}]}